{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q4: Neural Additive Models2 (7 Pts)\n",
    "Another way to make deep models more interpretable is by careful design of the architecture. One example of such a model is the Neural Additive Model (NAM), which is an instance of the class of Generalized Additive Models3 (GAM). Read the paper about NAMs, implement the model, and train it on the dataset (3 Pt). Like Q2-3, provide performance metrics on the test set. Utilize the interpretability of NAMs to visualize the feature importances (2 Pt). Conceptually, how does the model compare to Logistic Regression and MLPs (1 Pt)? Why are NAMs more interpretable than MLPs despite being based on non-linear neural networks (1 Pt)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Neural Additive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Neural Additive Model (NAM) using PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralAdditiveModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralAdditiveModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize NAM model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "nam_model = NeuralAdditiveModel(input_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(nam_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = nam_model(torch.FloatTensor(X_train_scaled))\n",
    "    loss = criterion(outputs, torch.FloatTensor(y_train.values).view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate model\n",
    "with torch.no_grad():\n",
    "    outputs = nam_model(torch.FloatTensor(X_test_scaled))\n",
    "    predictions = torch.round(torch.sigmoid(outputs)).numpy()\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
