{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Multi-Layer Perceptrons (5 Pts)\n",
    "While often reaching superior performance, MLPs are generally hard to interpret, and it is not straightforward to see what is happening within these models. We thus opt for post-hoc explainability methods such as SHAP1. Post-hoc explainability methods typically use some procedure during inference to find the feature importance per sample. Similar to Q2, implement a simple MLP, train it on the dataset, and report test set performance (2 Pt). Then, visualize SHAP explanations of the outputs of two positive and negative samples and feature importances of the overall model (2 Pt). Are feature importances consistent across different predictions and compared to overall importance values (1 Pt)? Elaborate on your findings! Hint: There is an excellent SHAP library for python that provides many SHAP algorithms and visualizations out of the box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train MLP classifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred_mlp))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
